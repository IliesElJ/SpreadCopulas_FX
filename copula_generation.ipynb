{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantBook Analysis Tool \n",
    "# For more information see [https://www.quantconnect.com/docs/v2/our-platform/research/getting-started]\n",
    "qb = QuantBook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull FX and CFD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "forex_pairs = ['CADUSD', 'CHFUSD', 'EURUSD', 'GBPUSD', 'JPYUSD', 'NZDUSD', 'AUDUSD']\n",
    "commodities = ['XAUUSD', 'XAGUSD', 'WTICOUSD', 'BCOUSD', 'CORNUSD', 'SOYBNUSD', 'XPDUSD', 'XPTUSD', 'NATGASUSD', 'SUGARUSD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def pull_qc_data(forex_pairs, commodities, n, resolution = Resolution.Hour, market = Market.Oanda):\n",
    "    history = []\n",
    "\n",
    "    # Fetch Forex data\n",
    "    for forex_pair in forex_pairs:\n",
    "        forex_ticker = qb.AddForex(forex_pair, resolution, market).Symbol\n",
    "        history.append(qb.History(forex_ticker, n, resolution).drop_duplicates())\n",
    "        print(f\"Pulling {forex_ticker}: done\")\n",
    "\n",
    "    # Fetch Commodity data\n",
    "    for commodity in commodities:\n",
    "        commodity_symbol = qb.AddCfd(commodity, resolution, market).Symbol\n",
    "        history.append(qb.History(commodity_symbol, n, resolution).drop_duplicates())\n",
    "        print(f\"Pulling {commodity_symbol}: done\")\n",
    "\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pull_qc_data(forex_pairs, commodities, 24*10000, Resolution.Hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def preprocess_qc_data(history, ffill=True):\n",
    "\n",
    "    history_dfs = pd.concat(history).reset_index()\n",
    "    history_dfs['time'] = pd.to_datetime(history_dfs['time'])\n",
    "    # Filter the pivoted DataFrame\n",
    "    if ffill:   \n",
    "        history_filt = history_dfs.pivot(index='time', columns='symbol', values='close').fillna(method='ffill')\n",
    "    else:   \n",
    "        history_filt = history_dfs.pivot(index='time', columns='symbol', values='close')\n",
    "\n",
    "    return history_filt\n",
    "\n",
    "def drop_most_nan_pairs(df, n=1):\n",
    "    # Count NaN values for each column and sort\n",
    "    nan_counts = df.isna().sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Get the 10 forex pairs with the most NaN values\n",
    "    most_nan_pairs = nan_counts.nlargest(n).index\n",
    "    \n",
    "    # Drop these pairs from the DataFrame\n",
    "    df = df.drop(columns=most_nan_pairs)\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def calculate_spread(df_train, df_val):\n",
    "    lr = LinearRegression()\n",
    "    spreads_train = pd.DataFrame(index=df_train.index)\n",
    "    spreads_val = pd.DataFrame(index=df_val.index)\n",
    "    \n",
    "    for symbol in df_train.columns:\n",
    "        if symbol == 'EURUSD':\n",
    "            continue\n",
    "        \n",
    "        # reshape for sklearn\n",
    "        X_train = df_train[symbol].values.reshape(-1,1)\n",
    "        y_train = df_train['EURUSD'].values.reshape(-1,1)\n",
    "\n",
    "        # fit the model\n",
    "        lr.fit(X_train, y_train)\n",
    "        \n",
    "        # use the coefficient to calculate spread\n",
    "        spreads_train[symbol] = df_train['EURUSD'] - lr.coef_[0] * df_train[symbol]\n",
    "        spreads_val[symbol] = df_val['EURUSD'] - lr.coef_[0] * df_val[symbol]\n",
    "\n",
    "    return spreads_train, spreads_val\n",
    "\n",
    "\n",
    "def split_data(df: pd.DataFrame) -> (List[pd.DataFrame], List[pd.DataFrame]):\n",
    "    \n",
    "    # Convert the index to DateTimeIndex if it's not already\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Sort the DataFrame by date\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Determine the number of months in the dataset\n",
    "    num_months = (df.index.max() - df.index.min()).days // 30\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "\n",
    "    # Split data into 4-month blocks with 1-month overlap and further into training and validation sets\n",
    "    for i in range(0, num_months, 1):  # change here\n",
    "        # Determine the cutoff dates for each period\n",
    "        start_date = df.index.min() + pd.DateOffset(months=i)\n",
    "        middle_date = start_date + pd.DateOffset(months=3)\n",
    "        end_date = start_date + pd.DateOffset(months=4)\n",
    "\n",
    "        # Split the data and append to the respective lists\n",
    "        train_data.append(df[(df.index >= start_date) & (df.index < middle_date)])\n",
    "        val_data.append(df[(df.index >= middle_date) & (df.index < end_date)])\n",
    "\n",
    "    return train_data, val_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fx_filt = preprocess_qc_data(history)\n",
    "history_fx_filt = drop_most_nan_pairs(history_fx_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history, validate_history = split_data(history_fx_filt.iloc[50000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_dates(val_data):\n",
    "    all_dates = []\n",
    "\n",
    "    # Iterate over all dataframes in val_data\n",
    "    for df in val_data:\n",
    "        all_dates.extend(df.index.tolist())\n",
    "    \n",
    "    # Check if the number of unique dates is equal to the total number of dates\n",
    "    return len(all_dates) == len(set(all_dates))\n",
    "\n",
    "print(f\"Sanity check for timestamps: {check_unique_dates(validate_history)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cointegrated pairs\n",
    "\n",
    "### Using Engle-Granger test and Kendall's tau to measure rank correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_engle_granger(forex_pairs, df):\n",
    "\n",
    "    cointegrated_pairs = []\n",
    "\n",
    "    for i in range(len(forex_pairs)):\n",
    "        for j in range(i+1, len(forex_pairs)):\n",
    "\n",
    "            forex_pair1 = forex_pairs[i]\n",
    "            forex_pair2 = forex_pairs[j]\n",
    "\n",
    "            eg_test_result = coint(df[forex_pair1], df[forex_pair2], trend='c', method='aeg', autolag='AIC')\n",
    "            eg_output = pd.Series(eg_test_result[0:3], index=['Test Statistic','p-value','Critical Values 1%,5%,10%'])\n",
    "\n",
    "            print(f'Results of Engle-Granger Test between {forex_pair1} and {forex_pair2}:')\n",
    "            print(eg_output)\n",
    "\n",
    "            if eg_test_result[1] < 0.1: # pairs with a p-value less than 0.01 are considered cointegrated\n",
    "                cointegrated_pairs.append((forex_pair1, forex_pair2))\n",
    "\n",
    "\n",
    "    return cointegrated_pairs\n",
    "\n",
    "# Function to compute Kendall's Tau\n",
    "def kendall_tau(x, y):\n",
    "    return kendalltau(x, y)[0]  # we only want the correlation coefficient\n",
    "\n",
    "def find_pair_with_highest_tau(df, pairs):\n",
    "    \"\"\"\n",
    "    Finds the pair of columns in the DataFrame with the highest Kendall's Tau.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame.\n",
    "        pairs (list): The list of pairs to consider.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The pair with the highest Kendall's Tau.\n",
    "    \"\"\"\n",
    "    max_tau = -1  # initial value (minimum possible)\n",
    "    best_pair = None\n",
    "\n",
    "    for pair in pairs:\n",
    "        tau = kendall_tau(df[pair[0]], df[pair[1]])\n",
    "        if tau > max_tau:\n",
    "            max_tau = tau\n",
    "            best_pair = pair\n",
    "\n",
    "    return best_pair\n",
    "\n",
    "def find_pair_with_highest_corr(df, pairs):\n",
    "    \"\"\"\n",
    "    Finds the pair of columns in the DataFrame with the highest Kendall's Tau.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame.\n",
    "        pairs (list): The list of pairs to consider.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The pair with the highest Kendall's Tau.\n",
    "    \"\"\"\n",
    "    max_tau = -1  # initial value (minimum possible)\n",
    "    best_pair = None\n",
    "\n",
    "    for pair in pairs:\n",
    "        tau = df[pair[0]].corr(df[pair[1]])\n",
    "        if tau > max_tau:\n",
    "            max_tau = tau\n",
    "            best_pair = pair\n",
    "\n",
    "    return best_pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the hedge ratio between EURUSD and the other pairs \n",
    "(Old approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "def calculate_spread(df_train, df_val):\n",
    "    lr = LinearRegression()\n",
    "    spreads_train = pd.DataFrame(index=df_train.index)\n",
    "    spreads_val = pd.DataFrame(index=df_val.index)\n",
    "    \n",
    "    for symbol in df_train.columns:\n",
    "        if symbol == 'EURUSD':\n",
    "            continue\n",
    "        \n",
    "        # reshape for sklearn\n",
    "        X_train = df_train[symbol].values.reshape(-1,1)\n",
    "        y_train = df_train['EURUSD'].values.reshape(-1,1)\n",
    "\n",
    "        # fit\n",
    "        lr.fit(X_train, y_train)\n",
    "        \n",
    "        # use the coefficient to calculate spread\n",
    "        spreads_train[symbol] = df_train['EURUSD'] - lr.coef_[0] * df_train[symbol]\n",
    "        spreads_val[symbol] = df_val['EURUSD'] - lr.coef_[0] * df_val[symbol]\n",
    "\n",
    "    return spreads_train, spreads_val\n",
    "    \n",
    "def get_ecdfs(data_1, data_2):\n",
    "\n",
    "    def get_ecdf(data, t):\n",
    "        n = len(data)\n",
    "        return sum([int(x < t) for x in data])/n\n",
    "\n",
    "    data_1_trsf = list(map(lambda x: get_ecdf(data_1, x), data_1))\n",
    "    data_2_trsf = list(map(lambda x: get_ecdf(data_2, x), data_2))\n",
    "\n",
    "    return data_1_trsf, data_2_trsf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the first 4-month block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = calculate_spread(train_history[0], validate_history[0])\n",
    "a, b = train_history[0], validate_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_spreads = run_engle_granger(a.columns, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_spread = find_pair_with_highest_tau(a, coint_spreads)\n",
    "copula_data = pd.concat([a[best_spread[0]], a[best_spread[1]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "copula_data = get_ecdfs(a[best_spread[0]], a[best_spread[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.bivariate import Gumbel\n",
    "\n",
    "class GaussianMultivariateCustom(Gumbel):\n",
    "    \n",
    "    def partial_derivative_u(self, u, v):\n",
    "        r\"\"\"Compute first partial derivative of cumulative distribution.\n",
    "\n",
    "        The partial derivative of the copula(CDF) is the conditional CDF.\n",
    "\n",
    "        .. math:: F(v|u) = \\frac{\\partial C(u,v)}{\\partial u} =\n",
    "            C(u,v)\\frac{((-\\ln u)^{\\theta} + (-\\ln v)^{\\theta})^{\\frac{1}{\\theta} - 1}}\n",
    "            {\\theta(- \\ln u)^{1 -\\theta}}\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray)\n",
    "            y (float)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "\n",
    "        \"\"\"\n",
    "        X = np.column_stack((u, v))\n",
    "\n",
    "        return self.partial_derivative(X)\n",
    "\n",
    "    def partial_derivative_v(self, u, v):\n",
    "            r\"\"\"Compute second partial derivative of cumulative distribution.\n",
    "\n",
    "            The partial derivative of the copula(CDF) is the conditional CDF.\n",
    "\n",
    "            .. math:: F(v|u) = \\frac{\\partial C(u,v)}{\\partial u} =\n",
    "                C(u,v)\\frac{((-\\ln u)^{\\theta} + (-\\ln v)^{\\theta})^{\\frac{1}{\\theta} - 1}}\n",
    "                {\\theta(- \\ln u)^{1 -\\theta}}\n",
    "\n",
    "            Args:\n",
    "                X (np.ndarray)\n",
    "                y (float)\n",
    "\n",
    "            Returns:\n",
    "                numpy.ndarray\n",
    "\n",
    "            \"\"\"\n",
    "            X = np.column_stack((v, u))\n",
    "\n",
    "            return self.partial_derivative(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "copula = GaussianMultivariateCustom()\n",
    "copula.fit(pd.DataFrame(copula_data).T.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_prob_u = copula.partial_derivative_u\n",
    "cond_prob_v = copula.partial_derivative_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the resolution of the grid (100x100 grid here)\n",
    "resolution = 100\n",
    "\n",
    "# Generate the grid over the range [0,1]Â²\n",
    "x = np.linspace(0, 1, resolution)\n",
    "y = np.linspace(0, 1, resolution)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "# Compute the value of the function at each point in the grid\n",
    "z_u = np.empty_like(xx)\n",
    "z_v = np.empty_like(xx)\n",
    "\n",
    "for i in range(resolution):\n",
    "    for j in range(resolution):\n",
    "        z_u[i, j] = cond_prob_u(xx[i, j], yy[i, j])\n",
    "        z_v[i, j] = cond_prob_v(xx[i, j], yy[i, j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(z_v).iloc[:, ::-1], cmap=\"YlOrBr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(a[best_spread[0]], a[best_spread[1]], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(copula_data[0], copula_data[1], s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "sns.heatmap(history_fx_filt.corr().apply(lambda x: round(x, 4)), annot=True, cmap=\"YlOrBr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.bivariate.gumbel import Gumbel\n",
    "from copulas.bivariate.utils import split_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "copula = Gumbel()\n",
    "copula.fit(pd.DataFrame(copula_data).T.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "copula.partial_derivative_scalar(0.2, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_matrix(pd.DataFrame(copula_data).T.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the copula's partial derivatives in QC ObjectStore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.bivariate import Gumbel\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def find_and_fit_best_spreads(train_histories, val_histories):\n",
    "\n",
    "    copula_series = {}\n",
    "    ecdfs_series = {}  # To store the ecdfs\n",
    "    best_spread_series = {}  # To store the best spreads\n",
    "    total = len(train_histories)\n",
    "\n",
    "    for i, (train_df, val_df) in enumerate(zip(train_histories, val_histories)):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        percentage_done = (i + 1) / total * 100\n",
    "        print(f\"Progress: {percentage_done:.2f}%\")\n",
    "\n",
    "        if len(val_df) > 0:\n",
    "            # Find cointegrated pairs\n",
    "            av_pairs = list(train_df.columns)\n",
    "            coint_pairs = run_engle_granger(av_pairs, train_df)\n",
    "\n",
    "            # Find pair with highest tau\n",
    "            best_spread = find_pair_with_highest_tau(train_df, coint_pairs)\n",
    "            \n",
    "            # Get empirical cumulative distribution functions of the spread\n",
    "            copula_data = get_ecdfs(train_df[best_spread[0]], train_df[best_spread[1]])\n",
    "            \n",
    "            try:\n",
    "                # Fit the copula\n",
    "                copula = Gumbel()\n",
    "                copula.fit(np.column_stack(copula_data))\n",
    "\n",
    "                # Store the fitted copula, the ecdfs and the best spread\n",
    "                copula_series[val_df.index[0]] = copula\n",
    "                ecdfs_series[val_df.index[0]] = (ECDF(train_df[best_spread[0]]), ECDF(train_df[best_spread[1]]))\n",
    "                best_spread_series[val_df.index[0]] = best_spread\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                copula_series[val_df.index[0]] = np.nan\n",
    "                ecdfs_series[val_df.index[0]] = np.nan\n",
    "                best_spread_series[val_df.index[0]] = np.nan\n",
    "    \n",
    "    return pd.Series(copula_series), pd.Series(ecdfs_series), pd.Series(best_spread_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "copula_series, ecdfs_series, best_spread_series = find_and_fit_best_spreads(train_history, validate_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "copula_series.iloc[0].partial_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Convert pandas Series to string, then encode to bytes for storage\n",
    "copula_series_bytes = pickle.dumps(copula_series)\n",
    "ecdfs_series_bytes = pickle.dumps(ecdfs_series)\n",
    "best_spread_series_bytes = pickle.dumps(best_spread_series)\n",
    "\n",
    "# Save data to Object Store\n",
    "qb.ObjectStore.SaveBytes(\"copula_series_key\", copula_series_bytes)\n",
    "qb.ObjectStore.SaveBytes(\"ecdfs_series_key\", ecdfs_series_bytes)\n",
    "qb.ObjectStore.SaveBytes(\"best_spread_series_key\", best_spread_series_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kvp in qb.ObjectStore:\n",
    "    key = kvp.Key\n",
    "    value = kvp.Value\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiprocessing (Incomplete)\n",
    "\n",
    "Can only be run on docker using Lean CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def process_data(args):\n",
    "\n",
    "    i, train_df, val_df = args\n",
    "\n",
    "    if len(val_df) > 0:\n",
    "\n",
    "        # Find cointegrated pairs \n",
    "        av_pairs = list(train_df.columns)\n",
    "        coint_pairs = run_engle_granger(av_pairs, train_df)\n",
    "\n",
    "        # Find pair with highest tau\n",
    "        best_spread = find_pair_with_highest_tau(train_df, coint_pairs)\n",
    "        \n",
    "        # Get empirical cumulative distribution functions of the spread\n",
    "        copula_data = get_ecdfs(train_df[best_spread[0]], train_df[best_spread[1]])\n",
    "        \n",
    "        try:\n",
    "            # Fit the copula\n",
    "            copula = GaussianMultivariateCustom()\n",
    "            copula.fit(np.column_stack(copula_data))\n",
    "\n",
    "            # Store the fitted copula, the ecdfs and the best spread\n",
    "            copula_series[val_df.index[0]] = copula\n",
    "            ecdfs_series[val_df.index[0]] = (ECDF(train_df[best_spread[0]]), ECDF(train_df[best_spread[1]]))\n",
    "            best_spread_series[val_df.index[0]] = best_spread\n",
    "\n",
    "        except ValueError:\n",
    "\n",
    "            copula_series[val_df.index[0]] = np.nan\n",
    "            ecdfs_series[val_df.index[0]] = np.nan\n",
    "            best_spread_series[val_df.index[0]] = np.nan    \n",
    "            \n",
    "        return (val_df.index[0], copula, ecdf, best_spread)\n",
    "\n",
    "def find_and_fit_best_spreads_parallel(train_histories, val_histories, num_processes=4):\n",
    "    with Pool(num_processes) as p:\n",
    "        results = p.map(process_data, enumerate(zip(train_histories, val_histories)))\n",
    "\n",
    "    copula_series = {k: v for k, v, _, _ in results}\n",
    "    ecdf_series = {k: v for k, _, v, _ in results}\n",
    "    best_spread_series = {k: v for k, _, _, v in results}\n",
    "\n",
    "    return pd.Series(copula_series), pd.Series(ecdf_series), pd.Series(best_spread_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from QC ObjectStore \n",
    "To build a sanity check function used in the backtesting framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# from gumbel_copula import GaussianMultivariateCustom\n",
    "\n",
    "def load_copulas(qb):\n",
    "    # Import the series from the Object Store\n",
    "    copula_series_object_ = qb.ObjectStore.ReadBytes(\"copula_series_key\")\n",
    "    ecdf_series_object_ = qb.ObjectStore.ReadBytes(\"ecdfs_series_key\")\n",
    "    best_spread_series_object_ = qb.ObjectStore.ReadBytes(\"best_spread_series_key\")\n",
    "\n",
    "    copula_series_ = pickle.loads(copula_series_object_)\n",
    "    ecdf_series_ =  pickle.loads(ecdf_series_object_)\n",
    "    best_spread_series_ = pickle.loads(best_spread_series_object_)\n",
    "\n",
    "    return copula_series_, ecdf_series_, best_spread_series_\n",
    "\n",
    "# copula_series.index = pd.to_datetime(copula_series.index)\n",
    "# ecdf_series.index = pd.to_datetime(ecdf_series.index)\n",
    "# best_spread_series.index = pd.to_datetime(best_spread_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = load_copulas(qb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation-Py-Default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
